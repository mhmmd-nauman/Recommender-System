 provide abstract characterization boosting algorithms  gradient decsent cost-functionals inner-product function  space. prove convergence functional-gradient-descent  algorithms weak conditions. previous theo-  retical results bounding generalization performance convex  combinations classifiers terms general cost functions  margin, present algorithm (DOOM II) performing  gradient descent optimization cost functions. Experiments  data sets UC Irvine repository demonstrate  DOOM II generally outperforms AdaBoost, especially high  noise situations, overfitting behaviour AdaBoost  predicted cost functions. 