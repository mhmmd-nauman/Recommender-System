 propose approach building finite memory predictive mod-  els similar spirit variable memory length Markov models (VLMMs).  models constructed transforming n-block structure  training sequence spatial structure points unit hypercube,  longer common suffix shared n-blocks,  closer lie point representations. transformation embodies  Markov assumption - n-blocks common suffixes likely  produce similar continuations. Finding set prediction contexts  formulated resource allocation problem solved vector quantizing  spatial n-block representation. compare model  classical variable memory length Markov models data sets  memory stochastic components. models  superior performance, yet, construction fully automatic,  shown problematic case VLMMs. 