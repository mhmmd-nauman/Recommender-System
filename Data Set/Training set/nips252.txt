  introduce framework simultaneous structure parameter learning  hidden-variable conditional probability models, based entropic prior solution  maximum posteriori (MAP) estimator. MAP estimate minimizes uncertainty  respects: cross-entropy model data; entropy model; entropy  data's descriptive statistics. Iterative estimation extinguishes weakly supported  parameters, compressing sparsifying model. Trimming operators accelerate  process removing excess parameters and, unlike pruning schemes, guarantee  increase posterior probability. Entropic estimation takes overcomplete random  model simplifies it, inducing structure relations hidden observed  variables. Applied hidden Markov models (HMMs), finds concise finite-state  machine representing hidden structure signal. entropically model music,  handwriting, video time-series, show resulting models highly concise,  structured, predictive, interpretable: Surviving states tend highly correlated  meaningful partitions data, surviving transitions provide low-perplexity  model signal dynamics. 