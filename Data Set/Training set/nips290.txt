  paper, address issues long-standing interest re-  inforcement learning literature. First, kinds performance guar-  antees Q-learning finite number actions?  Second, quantitative comparisons Q-learning  model-based (indirect) approaches, experience estimate  next-state distributions off-line iteration?  show Q-learning indirect approach enjoy  rapid convergence optimal policy function num-  ber transitions observed. particular, order  (Nlog(1/e)/e2)(log(N) + loglog(I/e)) transitions sufficient  algorithms optimal policy, idealized model  assumes observed transitions "well-mixed"  N-state MDP. Thus, approaches roughly sample  complexity. surprisingly, sample complexity  required model-based approach actually construct  approximation next-state distribution. result shows  amount memory required model-based approach closer  2.  approach, remove assumption observed tran-  sitions well-mixed, consider model transitions  determined fixed, arbitrary exploration policy. Bounds number  transitions required order achieve desired level performance  related stationary distribution mixing  policy. 