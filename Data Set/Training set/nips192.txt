  study approximation functions two-layer feedforward neu-  ral networks, focusing incremental algorithms greedily add  units, estimating single unit parameters stage. opposed  standard algorithms fixed architectures, optimization stage  performed small number parameters, mitigating  difficult numerical problems inherent high-dimensional non-linear op-  timization. establish upper bounds error incurred al-  gorithm, approximating functions Sobolev class,  extending previous results provided rates convergence  functions convex hulls functional spaces. comparing  results recently derived lower bounds, show greedy algo-  rithms nearly optimal. Combined estimation error results  greedy algorithms, strong case type approach. 