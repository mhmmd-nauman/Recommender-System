  Partially Observable Markov Decision Processes (POMDPs) constitute  important class reinforcement learning problems present  unique theoretical computational difficulties. absence  Markov property, popular reinforcement learning algorithms  Q-learning longer effective, memory-based methods  remove partial observability state-estimation notoriously  expensive. alternative approach seek stochastic memoryless  policy observation environment prescribes  probability distribution actions maximizes  average reward timestep. reinforcement learning algorithm  learns locally optimal stochastic memoryless policy  proposed Jaakkola, Singh Jordan, empirically verified.  present variation algorithm, discuss implementation,  demonstrate viability test problems. 