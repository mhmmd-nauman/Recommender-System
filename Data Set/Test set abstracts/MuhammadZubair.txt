   portable, extensible fast stochastic volatility model calibration multi many-core processors. Concurrency Computation: Practice Experience   Financial markets change precipitously, on-demand pricing risk models constantly recalibrated reduce risk. However, classes models computationally intensive robustly calibrate intraday prices – stochastic volatility models archetypal due non-convexity objective function. order accelerate procedure parallel implementation, financial application developers faced growing plethora low-level high-performance computing frameworks Open Multi-Processing, Open Computing Language, compute unified device architecture, single instruction multiple data intrinsics, forced trade-off performance versus portability, flexibility, modularity code required facilitate rapid in-house model development productionisation. paper describes acceleration stochastic volatility model calibration multi-core CPUs graphics processing units (GPUs) Xcelerit platform. adopting simple programming model, Xcelerit platform enables application developer write sequential, high-level C++ code, concern low-level high-performance computing frameworks. platform portability, flexibility, modularity required application developers. Speedups 30x 293x achieved Intel Xeon CPU NVIDIA Tesla K40 GPU, compared sequential CPU implementation. Xcelerit platform implementation shown equivalent performance low-level compute unified device architecture version. Overall, able reduce entire calibration process sequential implementation 6189 183.8 17.8 CPU GPU, respectively, requiring developer reimplement low-level high-performance computing frameworks.  Challenges matching secondary structures cryo-EM:   Cryo-electron microscopy fast emerging biophysical technique structural determination large protein complexes. atomic structures determined technique, challenging derive atomic structures density maps produced medium resolution suitable templates available. critical step structure determination protein chain threads 3-dimensional density map. dynamic programming method previously developed generate matches secondary structures density map protein sequence shortest paths related weighted graph. discuss challenges creation weighted graph explore heuristic methods solve problem matching secondary structures.  Memory-Efficient Parallel Simulation Electron Beam Dynamics GPUs.   Accurate simulation collective effects electron beams challenging computationally intractable problems accelerator physics. recently, researchers developed GPU-accelerated, high-fidelity simulation electron beam dynamics models collective effects accurately. simulation, however, heavily data-intensive memory-bound. particular, data-dependent, irregular memory access patterns control-flow collective effects computation phase simulation leads large number non-coalesced memory accesses GPU. significantly deteriorates performance. Moreover, parallel simulation exhibits poor data locality. This, non-coalesced memory accesses, leads ineffective memory hierarchy. present cache-aware algorithm locality heuristic maximize data reuse improving data locality. Additionally, algorithm control-flow heuristic balance workload threads. control-flow heuristic minimizes threads divergence enables reuse partial results previous iterations reducing operation count. Experimental results NVIDIA Tesla K40 GPU shows approach delivers 450 Gflops double precision performance, translates speedup 16X compared current state-of-the-art GPU implementation.  Optimized Multicolor Point-Implicit Solver Unstructured Grid Applications Graphics Processing Units.   field computational fluid dynamics, Navier-Stokes equations solved unstructured-grid approach accommodate geometric complexity. Implicit solution methodologies spatial discretizations generally require frequent solution large tightly-coupled systems block-sparse linear equations. multicolor point-implicit solver current typically requires significant fraction application run time. work, efficient implementation solver graphics processing units proposed. factors present unique challenges achieving efficient implementation environment. include variable amount parallelism kernel calls, indirect memory access patterns, low arithmetic intensity, requirement support variable block sizes. work, solver reformulated standard sparse dense Basic Linear Algebra Subprograms (BLAS) functions. However, experiments show performance BLAS functions existing CUDA libraries suboptimal matrices representative encountered actual simulations. Instead, optimized versions functions developed. Depending block size, implementations show performance gains 7× existing CUDA library functions. Optimizing Multiple Right-Hand Side Dslash Kernel Intel Knights Corner.  significant interest computational physics community perform lattice quantum chromodynamics (LQCD) simulations, run trillions operations. LQCD computations solve sparse linear system Wilson Dslash kernel, arithmetic intensity 0.88–2.29. makes Dslash memory bandwidth-bound architectures, including Intel Xeon Phi Knights Corner (KNC). research optimizing Dslash operator focused single right-hand side (SRHS) linear solvers. class LQCD computations aims solve systems multiple right-hand sides (MRHS), presenting additional opportunities data reuse vectorization. present approaches MRHS Dslash: vector register blocking approach software package QPhiX custom code generator low-level intrinsics. observed significant speedups approaches, sustained performance 700 GFLOPS (single precision) instance. achieved 29 % theoretical peak performance compared maximum 13 % obtained previous SRHS method QPhiX.  ISQuest: finding insertion sequences prokaryotic sequence fragment data.   Insertion sequences (ISs) transposable elements present bacterial archaeal genomes play important role genomic evolution. increasing availability sequenced prokaryotic genomes offers opportunity study ISs comprehensively, development efficient accurate tools required discovery annotation. Additionally, prokaryotic genomes frequently deposited incomplete, draft stage substantial cost effort required finish genome assembly projects. Development methods identify directly raw sequence reads draft genomes desirable. Software tools Optimized Annotation System Insertion Sequences IScan identify elements completely assembled annotated genomes; however, knowledge methods developed identify ISs raw fragment data partially assembled genomes. developed methods solve computationally challenging problem, implemented methods software package ISQuest. software identifies bacterial ISs sequence elements—inverted direct repeats—in raw read data contigs flexible search parameters. ISQuest capable finding ISs hundreds partially assembled genomes hours, making valuable high-throughput tool global search elements. tested ISQuest simulated read libraries 3810 complete bacterial genomes plasmids GenBank capable detecting 82% ISs transposases annotated GenBank 80% sequence identity. Dynamic Programming Algorithm Finding Optimal Placement Secondary Structure Topology Cryo-EM Data.  determination secondary structure topology critical step deriving atomic structures protein density maps obtained electron cryomicroscopy technique. step relies matching secondary structure traces detected protein density map secondary structure sequence segments predicted amino acid sequence. Due inaccuracies sources information, pool secondary structure positions needs sampled. approach problem derive small number topologies existing matching algorithms, find optimal placement topology. present dynamic programming method Θ(Nq2h) find optimal placement secondary structure topology. show algorithm requires significantly computational brute force method order Θ(qN h).  Computational Method Deriving Protein Secondary Structure Topologies Cryo-EM Density Maps Multiple Secondary Structure Predictions.   key idea de novo secondary structure topology determination methods calculate optimal mapping observed secondary structure traces Cryo-EM density image predicted secondary structures protein sequence. problem complicated presence multiple secondary structure predictions protein sequence (for predicted prediction methods). present computational method elegantly efficiently solves problem dealing multiple secondary structure predictions calculating optimal mapping. proposed method two-step approach – consensus positions secondary structures produce top topologies, dynamic programming method find optimal placement secondary structure traces density image. method tested twelve proteins types. observed rank true topologies consistently improved multiple secondary structure predictions single prediction. results show algorithm robust works presence errors/misses predicted secondary structures image sequence. results show algorithm efficient able handle proteins thirty-three helices.  Balsa Terzic: High-fidelity simulation collective effects electron beams innovative parallel method.   challenging heretofore unsolved problems accelerator physics accurate simulation collective effects electron beams. Electron beam dynamics crucial understanding design of: (i) high-brightness synchrotron light sources --- powerful tools cutting-edge research physics, biology, medicine fields, (ii) electron-ion particle colliders, probe nature matter unprecedented depths. Serial, naively parallel, implementation electron beam's self-interaction prohibitively costly terms efficiency memory requirements, necessitating simulation times order months years. paper, present innovative, high-performance, high-fidelity, scalable model simulation collective effects electron beams state-of-the-art multicore systems (GPUs, multicore CPUs, hybrid CPU-GPU platform). parallel simulation algorithm implemented multicore systems outperforms sequential simulation, achieving performance gain 7.7X 50X Intel Xeon E5630 CPU GTX 480 GPU, respectively. scales nearly linearly cluster size. simulation code scalable parallel implementation GPUs, multicore CPUs, hybrid CPU-GPU platform simulating collective dynamical effects electron beams accelerator physics.  Solving Secondary Structure MatchingProblem Cryo-EM De Novo ModelingUsing Constrained $K$-Shortest Path Graph Algorithm.   Electron cryomicroscopy major experimental technique solving structures large molecular assemblies. three-dimensional images obtained medium resolutions 10 Å. resolution range, major α-helices detected cylindrical sticks β-sheets detected plain-like regions. critical question de novo modeling cryo-EM images determine match detected secondary structures image protein sequence. formulate matching problem constrained graph problem present O(Δ2N22N) algorithm NP-Hard problem. algorithm incorporates dynamic programming approach constrained K-shortest path algorithm. method, DP-TOSS, tested α-proteins maximum 33 helices α-β proteins helices 12 β-strands. correct match ranked top 35 19 20 α-proteins α-β proteins tested. results demonstrate DP-TOSS improves accuracy, memory space deriving topologies secondary structure elements proteins large number secondary structures complex skeleton.  Big data challenges estimating genome assembler quality.   selection assembler important obtain assembly fragment dataset, avoid misassembles minimize finishing effort. known assembly quality assemblers dependent input data parameters DNA fragmentation parameters genome sequence structure. knowledge large scale systematic effort quantifying quality assembly generated assemblers range input parameters. correlation input parameters assembler quality define characteristics assembler design optimal assembler selection algorithm. critical barrier computational challenge assembling simulated high-throughput sequence libraries thousands genomes input parameters varied cover spectrum values obtained major sequencers biologists today. present study show quantifiable correlation drawn input output characteristics major open-source assemblers. Based result propose simple model estimate quality assemblies generated assemblers input parameters.  efficient algorithm k-core decomposition multicore processors.   k-core graph largest induced subgraph minimum degree k. k-core decomposition find core number vertex graph, largest vertex belongs k-core. k-core decomposition applications areas including network analysis, computational biology graph visualization. primary reason widely availability O(n + m) algorithm. algorithm proposed Batagelj Zaversnik considered state-of-the-art algorithm k-core decomposition. However, algorithm suitable parallelization knowledge algorithm proposed k-core decomposition multicore processors. Also, algorithm experimentally analyzed large graphs. working set size algorithm large, access pattern highly random, inefficient large graphs. paper, present experimental analysis algorithm Batagelj Zaversnik propose algorithm, ParK, significantly reduces working set size minimizes random accesses. provide experimental analysis algorithm graphs 65 million vertices 1.8 billion edges. compare ParK algorithm state-of-the-art algorithm show times faster. provide parallel methodology show algorithm amenable parallelization multicore architectures. ran experiments socket Nehalem-EX processor cores socket show algorithm scales 21 times 32 cores.  bit-based approach maximal clique enumeration multicore processors.   Maximal clique enumeration (MCE) fundamental problem graph theory. plays vital role network analysis applications computational biology. MCE extensively studied problem. Recently, Eppstein proposed state-of-the-art sequential algorithm degeneracy based ordering vertices improve efficiency. paper, propose parallel implementation algorithm Eppstein bit-based data structure. data structure reduces working set size significantly enabling bit-parallelism improves performance algorithm. illustrate significance degeneracy ordering load balancing experimentally evaluate impact scheduling performance algorithm. present experimental results types synthetic real-world graphs 50 million vertices 100 million edges. show approach outperforms Eppstein al.'s approach times scales 29 times run multicore machine 32 cores. Maximal clique enumeration large graphs hadoop framework.  Maximal clique enumeration (MCE) problem large graphs appears critical applications community detection social networks, aligning 3D protein sequences, finding motifs genomic data, identifying co-expressed genes data analytics communication networks. unusual graphs billions nodes edges applications. MCE problem NP hard, number algorithms sequential parallel proposed efficiently real graphs. addition large sizes input graphs, MCE algorithms general result large intermediate data making challenging efficiently process data. Recently approach proposed, referred pbitMCE, shown outperform perform equally compared existing approaches. approach degeneracy ordering vertices plays vital role performance algorithm. Degeneracy ordering vertices generated linear time. challenging find degeneracy ordering distributed environment requires extensive communication nodes. cases generating ordering significant amount time. cases ordering ordering degree choice degeneracy ordering. paper experimentally study impact ordering vertices performance MCE algorithm context mapreduce framework. present implementation pbitMCE mapreduce takes large graph ordering vertices input enumerates maximal cliques. support study, present experimental results graphs orderings. results show degree ordering performs comparable degeneracy ordering cases performs poorer case large graphs.  portable fast stochastic volatility model calibration multi many-core processors.   Financial markets change precipitously on-demand pricing risk models constantly recalibrated reduce risk. However, classes models computationally intensive robustly calibrate intraday prices-stochastic volatility models archetypal due non-convexity objective function. order accelerate procedure parallel implementation, financial application developers faced growing plethora low-level high-performance computing frameworks OpenMP, OpenCL, CUDA, SIMD intrinsics, forced trade-off performance versus portability, flexibility modularity code required facilitate rapid in-house model development productionization. Accelerating option risk analytics GPUs. Broadly, major prerequisite analytics applications robustness modeling idiosyncrasies. result, demand comprehensive model exploration validation high level statistical programming environments R. financial applications require on-demand processing, turn requires fast modeling calibration computations. paper describe speeding calibration Heston stochastic volatility model, financial application, GPUs. Heston volatility model extensively capital markets price measure market risk exchange traded financial options. However, typical based implementation Heston model calibration CPU meet performance requirements sub-minute level trading, i.e. mid high frequency trading. calibration Heston model performed option data points remains fixed calibration computation. computation intensive part computation ErrorFunction() estimates error market observed model option prices. 